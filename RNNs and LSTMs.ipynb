{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aefffe7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaypr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Downloading IMDB via kagglehub…\n",
      "[info] kagglehub path: C:\\Users\\jaypr\\.cache\\kagglehub\\datasets\\lakshmi25npathi\\imdb-dataset-of-50k-movie-reviews\\versions\\1\n",
      "[info] Building vocabulary…\n",
      "[info] Vocab size: 64,729\n",
      "[info] Trying GloVe via kagglehub: danielwillgeorge/glove6b100dtxt\n",
      "[info] Loaded 400,000 GloVe tokens.\n",
      "[info] GloVe coverage: 52213/64729 = 80.66%\n",
      "\n",
      "========================================================================\n",
      "[run] RNN + GloVe\n",
      "========================================================================\n",
      "[epoch 01] train_loss=0.6928 | val_loss=0.6890 acc=0.5356 f1=0.4708 auroc=0.5585\n",
      "[epoch 02] train_loss=0.6864 | val_loss=0.6823 acc=0.5668 f1=0.6314 auroc=0.5916\n",
      "[test] loss=0.6852 acc=0.5578 f1=0.6243 auroc=0.5826\n",
      "[info] Saved -> artifacts_pt_cpu\\RNN__GloVe.pt\n",
      "\n",
      "========================================================================\n",
      "[run] LSTM + GloVe\n",
      "========================================================================\n",
      "[epoch 01] train_loss=0.6920 | val_loss=0.6912 acc=0.5476 f1=0.5000 auroc=0.5718\n",
      "[epoch 02] train_loss=0.6906 | val_loss=0.6900 acc=0.5612 f1=0.5302 auroc=0.5888\n",
      "[test] loss=0.6908 acc=0.5476 f1=0.5138 auroc=0.5700\n",
      "[info] Saved -> artifacts_pt_cpu\\LSTM__GloVe.pt\n",
      "\n",
      "========================================================================\n",
      "[run] RNN + Trainable\n",
      "========================================================================\n",
      "[epoch 01] train_loss=0.6974 | val_loss=0.6914 acc=0.5272 f1=0.5236 auroc=0.5407\n",
      "[epoch 02] train_loss=0.6905 | val_loss=0.6877 acc=0.5440 f1=0.5634 auroc=0.5595\n",
      "[test] loss=0.6905 acc=0.5336 f1=0.5503 auroc=0.5445\n",
      "[info] Saved -> artifacts_pt_cpu\\RNN__Trainable.pt\n",
      "\n",
      "========================================================================\n",
      "[run] LSTM + Trainable\n",
      "========================================================================\n",
      "[epoch 01] train_loss=0.6945 | val_loss=0.6933 acc=0.5064 f1=0.5493 auroc=0.5056\n",
      "[epoch 02] train_loss=0.6931 | val_loss=0.6926 acc=0.5140 f1=0.5163 auroc=0.5201\n",
      "[test] loss=0.6931 acc=0.5012 f1=0.5451 auroc=0.5065\n",
      "[info] Saved -> artifacts_pt_cpu\\LSTM__Trainable.pt\n",
      "\n",
      "=== Summary ===\n",
      "           model  test_loss  test_acc  test_f1  test_auroc\n",
      "     RNN + GloVe     0.6852    0.5578   0.6243      0.5826\n",
      "    LSTM + GloVe     0.6908    0.5476   0.5138      0.5700\n",
      " RNN + Trainable     0.6905    0.5336   0.5503      0.5445\n",
      "LSTM + Trainable     0.6931    0.5012   0.5451      0.5065\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# IMDB Sentiment (PyTorch, CPU-only, no torchtext) — RNN/LSTM with GloVe + trainable embeddings\n",
    "# Tasks satisfied:\n",
    "#   1) GloVe + Vanilla RNN\n",
    "#   2) GloVe + LSTM\n",
    "#   3) Repeat [1] & [2] with on-the-fly trainable embeddings (nn.Embedding)\n",
    "\n",
    "# ---------- Environment: ensure CPU-only Torch 2.3.1 (avoids Windows autograd/optimizer quirks) ----------\n",
    "import os, sys, subprocess, importlib\n",
    "def _pip_install(pkgs, extra_args=None):\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-U\", *pkgs]\n",
    "    if extra_args: cmd.extend(extra_args)\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "def ensure_cpu_torch():\n",
    "    try:\n",
    "        import torch\n",
    "        ver = getattr(torch, \"__version__\", \"\")\n",
    "        has_cuda = getattr(torch.version, \"cuda\", None) is not None\n",
    "        if has_cuda or not ver.startswith(\"2.3.1\"):\n",
    "            raise RuntimeError(\"Switching to CPU wheels for stability\")\n",
    "    except Exception:\n",
    "        subprocess.call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"torch\", \"torchvision\", \"torchaudio\"])\n",
    "        subprocess.call([sys.executable, \"-m\", \"pip\", \"cache\", \"purge\"])\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\",\n",
    "                               \"--index-url\", \"https://download.pytorch.org/whl/cpu\",\n",
    "                               \"torch==2.3.1+cpu\", \"torchvision==0.18.1+cpu\", \"torchaudio==2.3.1+cpu\"])\n",
    "        # Restart so the new wheels load cleanly\n",
    "        os._exit(0)\n",
    "\n",
    "ensure_cpu_torch()\n",
    "\n",
    "# ---------- Deps ----------\n",
    "for p in [\"kagglehub\", \"pandas\", \"scikit-learn\"]:\n",
    "    try: importlib.import_module(p if p!=\"scikit-learn\" else \"sklearn\")\n",
    "    except ImportError: _pip_install([p])\n",
    "\n",
    "# ---------- Imports ----------\n",
    "import re, time, random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "\n",
    "# ---------- Config ----------\n",
    "SEED           = 42\n",
    "BATCH_SIZE     = 256\n",
    "EPOCHS         = 2              # bump to 3–5 for better scores\n",
    "MAX_LEN        = 150\n",
    "EMB_DIM        = 100            # GloVe 6B-100d\n",
    "HIDDEN_DIM     = 128\n",
    "LAYERS         = 1\n",
    "BIDIRECTIONAL  = True\n",
    "DROPOUT        = 0.2\n",
    "MIN_FREQ       = 2\n",
    "LR             = 2e-3\n",
    "NUM_WORKERS    = 0\n",
    "DEVICE         = torch.device(\"cpu\")  # stay CPU-only for stability\n",
    "SAVE_DIR       = Path(\"./artifacts_pt_cpu\"); SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Small speed nicety\n",
    "if hasattr(torch, \"set_float32_matmul_precision\"):\n",
    "    try: torch.set_float32_matmul_precision(\"high\")\n",
    "    except: pass\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# ---------- Basic text utils ----------\n",
    "def basic_english(text: str) -> List[str]:\n",
    "    return re.findall(r\"[a-z0-9']+\", str(text).lower())\n",
    "\n",
    "def pad_sequences(seqs, pad_idx=0):\n",
    "    lengths = torch.tensor([max(1, len(s)) for s in seqs], dtype=torch.long)\n",
    "    max_len = int(lengths.max().item()) if len(lengths) else 1\n",
    "    out = torch.full((len(seqs), max_len), pad_idx, dtype=torch.long)\n",
    "    for i, s in enumerate(seqs):\n",
    "        if len(s): out[i, :len(s)] = torch.tensor(s, dtype=torch.long)\n",
    "    return out, lengths\n",
    "\n",
    "def bucket_sort_by_length(examples, bucket_size=50):\n",
    "    buckets = {}\n",
    "    for ex in examples:\n",
    "        L = len(ex[\"input_ids\"]); key = (L // bucket_size) * bucket_size\n",
    "        buckets.setdefault(key, []).append(ex)\n",
    "    ordered = []\n",
    "    for k in sorted(buckets.keys()):\n",
    "        ordered.extend(buckets[k])\n",
    "    return ordered\n",
    "\n",
    "# ---------- Data ----------\n",
    "def load_imdb():\n",
    "    print(\"[info] Downloading IMDB via kagglehub…\")\n",
    "    path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
    "    print(\"[info] kagglehub path:\", path)\n",
    "    csv_path = Path(path) / \"IMDB Dataset.csv\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.sample(frac=1.0, random_state=123).reset_index(drop=True)\n",
    "    train_df = df.iloc[:40000].reset_index(drop=True)\n",
    "    val_df   = df.iloc[40000:45000].reset_index(drop=True)\n",
    "    test_df  = df.iloc[45000:].reset_index(drop=True)\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train_df, val_df, test_df = load_imdb()\n",
    "\n",
    "def build_vocab(tokenizer, texts, min_freq=MIN_FREQ, specials=(\"<pad>\", \"<unk>\")) -> Dict[str,int]:\n",
    "    freq = {}\n",
    "    for t in texts:\n",
    "        for tok in tokenizer(t):\n",
    "            if re.search(r\"[a-z0-9']\", tok):\n",
    "                freq[tok] = freq.get(tok, 0) + 1\n",
    "    vocab = {specials[0]:0, specials[1]:1}\n",
    "    idx = len(vocab)\n",
    "    for tok, c in sorted(freq.items(), key=lambda x: (-x[1], x[0])):\n",
    "        if c >= min_freq and tok not in vocab:\n",
    "            vocab[tok] = idx; idx += 1\n",
    "    return vocab\n",
    "\n",
    "print(\"[info] Building vocabulary…\")\n",
    "vocab = build_vocab(basic_english, train_df[\"review\"].tolist(), min_freq=MIN_FREQ)\n",
    "pad_idx, unk_idx = vocab[\"<pad>\"], vocab[\"<unk>\"]\n",
    "print(f\"[info] Vocab size: {len(vocab):,}\")\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, rows, max_len, vocab, tokenizer):\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_idx = vocab[\"<pad>\"]; self.unk_idx = vocab[\"<unk>\"]\n",
    "        self.data = []\n",
    "        for _, r in rows.iterrows():\n",
    "            text = str(r[\"review\"])\n",
    "            tokens = [t for t in tokenizer(text) if re.search(r\"[a-z0-9']\", t)]\n",
    "            tokens = tokens[:max_len]\n",
    "            if not tokens: tokens = [\"<unk>\"]\n",
    "            ids = [vocab.get(t, self.unk_idx) for t in tokens]\n",
    "            label = 1 if str(r[\"sentiment\"]).lower() == \"positive\" else 0\n",
    "            self.data.append({\"input_ids\": ids, \"label\": label})\n",
    "        self.data = bucket_sort_by_length(self.data, bucket_size=50)\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        return item[\"input_ids\"], item[\"label\"]\n",
    "    def collate(self, batch):\n",
    "        seqs, labels = zip(*batch)\n",
    "        x, lengths = pad_sequences(seqs, pad_idx=self.pad_idx)\n",
    "        y = torch.tensor(labels, dtype=torch.float32)\n",
    "        return x, lengths, y\n",
    "\n",
    "train_ds = IMDBDataset(train_df, MAX_LEN, vocab, basic_english)\n",
    "val_ds   = IMDBDataset(val_df,   MAX_LEN, vocab, basic_english)\n",
    "test_ds  = IMDBDataset(test_df,  MAX_LEN, vocab, basic_english)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          collate_fn=train_ds.collate, num_workers=NUM_WORKERS)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          collate_fn=val_ds.collate, num_workers=NUM_WORKERS)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          collate_fn=test_ds.collate, num_workers=NUM_WORKERS)\n",
    "\n",
    "# ---------- (Optional) GloVe ----------\n",
    "def maybe_load_glove_100d():\n",
    "    slugs = [\n",
    "        \"danielwillgeorge/glove6b100dtxt\",\n",
    "        \"parthplc/glove6b100dtxt\",\n",
    "        \"anindya2906/glove6b\",\n",
    "    ]\n",
    "    for slug in slugs:\n",
    "        try:\n",
    "            print(f\"[info] Trying GloVe via kagglehub: {slug}\")\n",
    "            gpath = kagglehub.dataset_download(slug)\n",
    "            for name in [\"glove.6B.100d.txt\", \"glove.6b.100d.txt\"]:\n",
    "                txt = Path(gpath) / name\n",
    "                if txt.exists():\n",
    "                    stoi, vecs = {}, []\n",
    "                    with open(txt, \"r\", encoding=\"utf-8\") as f:\n",
    "                        for line in f:\n",
    "                            parts = line.rstrip().split(\" \")\n",
    "                            if len(parts) < 101:  # word + 100 dims\n",
    "                                continue\n",
    "                            w = parts[0]\n",
    "                            vec = np.asarray(parts[1:], dtype=np.float32)\n",
    "                            if vec.shape[0] != 100: continue\n",
    "                            stoi[w] = len(stoi)\n",
    "                            vecs.append(torch.from_numpy(vec))\n",
    "                    if vecs:\n",
    "                        print(f\"[info] Loaded {len(stoi):,} GloVe tokens.\")\n",
    "                        return stoi, torch.stack(vecs)\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] GloVe fetch failed for {slug}: {e}\")\n",
    "    print(\"[warn] Could not load GloVe 6B-100d. Will skip GloVe-based models.\")\n",
    "    return None, None\n",
    "\n",
    "def build_embedding_matrix(vocab, glove_stoi, glove_vecs, dim=100):\n",
    "    mat = torch.randn(len(vocab), dim) * 0.05\n",
    "    mat[vocab[\"<pad>\"]] = torch.zeros(dim)\n",
    "    if glove_stoi is None or glove_vecs is None:\n",
    "        return mat\n",
    "    hit = 0\n",
    "    for tok, idx in vocab.items():\n",
    "        gi = glove_stoi.get(tok)\n",
    "        if gi is not None:\n",
    "            mat[idx] = glove_vecs[gi]; hit += 1\n",
    "    print(f\"[info] GloVe coverage: {hit}/{len(vocab)} = {100.0*hit/len(vocab):.2f}%\")\n",
    "    return mat\n",
    "\n",
    "glove_stoi, glove_vecs = maybe_load_glove_100d()\n",
    "emb_matrix = None\n",
    "if glove_stoi is not None and glove_vecs is not None:\n",
    "    emb_matrix = build_embedding_matrix(vocab, glove_stoi, glove_vecs, dim=EMB_DIM)\n",
    "\n",
    "# ---------- Models ----------\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, bidirectional,\n",
    "                 dropout, pad_idx, pretrained_emb=None, trainable_embed=False, cell=\"rnn\"):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        if pretrained_emb is not None:\n",
    "            with torch.no_grad(): self.embedding.weight.copy_(pretrained_emb)\n",
    "        self.embedding.weight.requires_grad = trainable_embed\n",
    "        rnn_drop = dropout if num_layers > 1 else 0.0\n",
    "        if cell == \"rnn\":\n",
    "            self.rnn = nn.RNN(emb_dim, hidden_dim, num_layers=num_layers,\n",
    "                              batch_first=True, bidirectional=bidirectional, dropout=rnn_drop)\n",
    "        elif cell == \"lstm\":\n",
    "            self.rnn = nn.LSTM(emb_dim, hidden_dim, num_layers=num_layers,\n",
    "                               batch_first=True, bidirectional=bidirectional, dropout=rnn_drop)\n",
    "        else:\n",
    "            raise ValueError(\"cell must be 'rnn' or 'lstm'\")\n",
    "        self.cell = cell\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        out_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "        self.fc = nn.Linear(out_dim, 1)\n",
    "    def forward(self, x, lengths):\n",
    "        emb = self.embedding(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        if self.cell == \"lstm\":\n",
    "            _, (h, _) = self.rnn(packed)\n",
    "        else:\n",
    "            _, h = self.rnn(packed)\n",
    "        if self.rnn.bidirectional:\n",
    "            h = torch.cat([h[-2], h[-1]], dim=1)\n",
    "        else:\n",
    "            h = h[-1]\n",
    "        h = self.dropout(h)\n",
    "        return self.fc(h).squeeze(1)\n",
    "\n",
    "# ---------- Train/Eval ----------\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    ys, ps, losses = [], [], []\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    for x, lengths, y in loader:\n",
    "        x, lengths, y = x.to(device), lengths.to(device), y.to(device)\n",
    "        logits = model(x, lengths)\n",
    "        loss = criterion(logits, y)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        ys.extend(y.cpu().numpy().tolist()); ps.extend(probs.tolist()); losses.append(loss.item())\n",
    "    ys = np.array(ys); ps = np.array(ps)\n",
    "    preds = (ps >= 0.5).astype(int)\n",
    "    acc = accuracy_score(ys, preds)\n",
    "    f1  = f1_score(ys, preds)\n",
    "    try: auroc = roc_auc_score(ys, ps)\n",
    "    except: auroc = float(\"nan\")\n",
    "    return float(np.mean(losses)), acc, f1, auroc\n",
    "\n",
    "def train_one(model, train_loader, val_loader, device, epochs=EPOCHS, lr=LR,\n",
    "              max_grad_norm=1.0, early_stopping_patience=2):\n",
    "    optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                lr=lr, momentum=0.9, nesterov=True)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    best_f1, best_state, no_improve = -1.0, None, 0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train(); losses=[]\n",
    "        for x, lengths, y in train_loader:\n",
    "            x, lengths, y = x.to(device), lengths.to(device), y.to(device)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            logits = model(x, lengths)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        val_loss, val_acc, val_f1, val_auroc = evaluate(model, val_loader, device)\n",
    "        print(f\"[epoch {epoch:02d}] train_loss={np.mean(losses):.4f} | \"\n",
    "              f\"val_loss={val_loss:.4f} acc={val_acc:.4f} f1={val_f1:.4f} auroc={val_auroc:.4f}\")\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= early_stopping_patience:\n",
    "                print(\"[info] Early stopping.\"); break\n",
    "    if best_state is not None: model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "# ---------- Orchestrate 4 runs ----------\n",
    "configs = []\n",
    "if emb_matrix is not None:\n",
    "    configs.append((\"RNN + GloVe\",  dict(cell=\"rnn\",  pretrained=emb_matrix, trainable=False)))\n",
    "    configs.append((\"LSTM + GloVe\", dict(cell=\"lstm\", pretrained=emb_matrix, trainable=False)))\n",
    "else:\n",
    "    print(\"[warn] Skipping GloVe models (vectors unavailable).\")\n",
    "configs.append((\"RNN + Trainable\",  dict(cell=\"rnn\",  pretrained=None, trainable=True)))\n",
    "configs.append((\"LSTM + Trainable\", dict(cell=\"lstm\", pretrained=None, trainable=True)))\n",
    "\n",
    "results = []\n",
    "for name, cfg in configs:\n",
    "    print(\"\\n\" + \"=\"*72); print(f\"[run] {name}\"); print(\"=\"*72)\n",
    "    model = RNNClassifier(\n",
    "        vocab_size=len(vocab),\n",
    "        emb_dim=EMB_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        num_layers=LAYERS,\n",
    "        bidirectional=BIDIRECTIONAL,\n",
    "        dropout=DROPOUT,\n",
    "        pad_idx=pad_idx,\n",
    "        pretrained_emb=cfg[\"pretrained\"],\n",
    "        trainable_embed=cfg[\"trainable\"],\n",
    "        cell=cfg[\"cell\"],\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    model = train_one(model, train_loader, val_loader, DEVICE, epochs=EPOCHS, lr=LR,\n",
    "                      max_grad_norm=1.0, early_stopping_patience=2)\n",
    "    tl, ta, tf1, auc = evaluate(model, test_loader, DEVICE)\n",
    "    print(f\"[test] loss={tl:.4f} acc={ta:.4f} f1={tf1:.4f} auroc={auc:.4f}\")\n",
    "\n",
    "    out_path = SAVE_DIR / (name.replace(\" \",\"_\").replace(\"+\",\"\").replace(\"/\",\"-\") + \".pt\")\n",
    "    torch.save({\"model_state\": model.state_dict(), \"vocab\": vocab}, out_path)\n",
    "    print(f\"[info] Saved -> {out_path}\")\n",
    "    results.append({\"model\": name, \"test_loss\": round(tl,4), \"test_acc\": round(ta,4),\n",
    "                    \"test_f1\": round(tf1,4), \"test_auroc\": round(auc,4)})\n",
    "\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(pd.DataFrame(results).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42344e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[done] Read 100 rows from: C:/Users/jaypr/Downloads/date_parser_testcases (1).csv\n",
      "[done] Wrote results to: parsed_dates_output.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ======== SET YOUR CSV PATH HERE ========\n",
    "CSV_PATH = r\"C:/Users/jaypr/Downloads/date_parser_testcases (1).csv\"\n",
    "# Example:\n",
    "# CSV_PATH = r\"C:\\Users\\you\\Downloads\\date_parser_testcases.csv\"\n",
    "# ========================================\n",
    "\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# -----------------------------\n",
    "# Month dictionary & utilities\n",
    "# -----------------------------\n",
    "_MONTHS = {\n",
    "    \"january\": 1, \"jan\": 1,\n",
    "    \"february\": 2, \"feb\": 2,\n",
    "    \"march\": 3, \"mar\": 3,\n",
    "    \"april\": 4, \"apr\": 4,\n",
    "    \"may\": 5,\n",
    "    \"june\": 6, \"jun\": 6,\n",
    "    \"july\": 7, \"jul\": 7,\n",
    "    \"august\": 8, \"aug\": 8,\n",
    "    \"september\": 9, \"sept\": 9, \"sep\": 9,\n",
    "    \"october\": 10, \"oct\": 10,\n",
    "    \"november\": 11, \"nov\": 11,\n",
    "    \"december\": 12, \"dec\": 12,\n",
    "}\n",
    "# Build a regex group for months (longer keys first to avoid partial greedy matches)\n",
    "_MONTH_KEYS_ORDERED = sorted(_MONTHS.keys(), key=len, reverse=True)\n",
    "_MONTH_RE = r\"(?:%s)\" % \"|\".join(map(re.escape, _MONTH_KEYS_ORDERED))\n",
    "\n",
    "# Common ordinal suffixes\n",
    "_ORDINAL_SUFFIX_RE = r\"(?:st|nd|rd|th)\"\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def _to_int(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _expand_year(y):\n",
    "    \"\"\"Expand 2-digit year to 4-digit using a simple rule:\n",
    "       00–49 → 2000–2049, 50–99 → 1950–1999\n",
    "    \"\"\"\n",
    "    if y is None:\n",
    "        return None\n",
    "    if y >= 100:\n",
    "        return y\n",
    "    return 2000 + y if y <= 49 else 1900 + y\n",
    "\n",
    "def _valid_date(y, m, d):\n",
    "    try:\n",
    "        datetime(y, m, d)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def _fmt_ddmmyyyy(y, m, d):\n",
    "    return f\"{d:02d}/{m:02d}/{y:04d}\"\n",
    "\n",
    "def _month_to_num(s):\n",
    "    if s is None:\n",
    "        return None\n",
    "    key = s.strip().lower().rstrip(\".\")  # allow trailing period in \"Sep.\"\n",
    "    return _MONTHS.get(key)\n",
    "\n",
    "def _strip_ordinals(text):\n",
    "    # Replace \"21st\", \"3rd\" → \"21\", \"3\"\n",
    "    return re.sub(rf\"\\b(\\d{{1,2}}){_ORDINAL_SUFFIX_RE}\\b\", r\"\\1\", text, flags=re.IGNORECASE)\n",
    "\n",
    "# -----------------------------\n",
    "# Regex patterns (compiled)\n",
    "# -----------------------------\n",
    "FLAGS = re.IGNORECASE\n",
    "\n",
    "# Textual month patterns\n",
    "PAT_TXT_DMY = re.compile(\n",
    "    rf\"\\b(?P<d>\\d{{1,2}})(?:{_ORDINAL_SUFFIX_RE})?(?:\\s+of)?[\\s,\\-\\/]*\"\n",
    "    rf\"(?P<month>{_MONTH_RE})\\.?[\\s,\\-\\/]*,?\\s*(?:'|’)?(?P<y>\\d{{2,4}})\\b\", FLAGS\n",
    ")\n",
    "\n",
    "PAT_TXT_MDY = re.compile(\n",
    "    rf\"\\b(?P<month>{_MONTH_RE})\\.?[\\s,\\-\\/]*\"\n",
    "    rf\"(?P<d>\\d{{1,2}})(?:{_ORDINAL_SUFFIX_RE})?[\\s,\\-\\/]*,?\\s*(?:'|’)?(?P<y>\\d{{2,4}})\\b\", FLAGS\n",
    ")\n",
    "\n",
    "PAT_TXT_YMD = re.compile(\n",
    "    rf\"\\b(?P<y>\\d{{4}})[\\s,\\-\\/]+(?P<month>{_MONTH_RE})\\.?[\\s,\\-\\/]+\"\n",
    "    rf\"(?P<d>\\d{{1,2}})(?:{_ORDINAL_SUFFIX_RE})?\\b\", FLAGS\n",
    ")\n",
    "\n",
    "# ISO-like numeric: YYYY-MM-DD / YYYY/MM/DD / YYYY.MM.DD\n",
    "PAT_ISO_YMD = re.compile(\n",
    "    r\"\\b(?P<y>\\d{4})[./-](?P<m>\\d{1,2})[./-](?P<d>\\d{1,2})\\b\"\n",
    ")\n",
    "\n",
    "# Numeric D/M/Y with separators ./- or spaces\n",
    "PAT_NUM_DMY = re.compile(\n",
    "    r\"\\b(?P<d>\\d{1,2})[.\\-\\/ ](?P<m>\\d{1,2})[.\\-\\/ ](?P<y>\\d{2,4})\\b\"\n",
    ")\n",
    "\n",
    "# Numeric M/D/Y with separators ./- or spaces\n",
    "PAT_NUM_MDY = re.compile(\n",
    "    r\"\\b(?P<m>\\d{1,2})[.\\-\\/ ](?P<d>\\d{1,2})[.\\-\\/ ](?P<y>\\d{2,4})\\b\"\n",
    ")\n",
    "\n",
    "# Compact textual like \"21Jun2024\" or \"Jun21'24\"\n",
    "PAT_TXT_COMPACT_DMY = re.compile(\n",
    "    rf\"\\b(?P<d>\\d{{1,2}})(?P<month>{_MONTH_RE})\\.?(?:'|’)?(?P<y>\\d{{2,4}})\\b\", FLAGS\n",
    ")\n",
    "PAT_TXT_COMPACT_MDY = re.compile(\n",
    "    rf\"\\b(?P<month>{_MONTH_RE})\\.?(?P<d>\\d{{1,2}})(?:'|’)?(?P<y>\\d{{2,4}})\\b\", FLAGS\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Core parsing logic\n",
    "# -----------------------------\n",
    "def _try_build_date_from_parts(y_str=None, m_str=None, d_str=None, month_name=None):\n",
    "    \"\"\"Return (dd/mm/yyyy) or None.\"\"\"\n",
    "    # month name → number\n",
    "    if month_name:\n",
    "        m = _month_to_num(month_name)\n",
    "    else:\n",
    "        m = _to_int(m_str)\n",
    "\n",
    "    d = _to_int(d_str)\n",
    "    y = _to_int(y_str)\n",
    "\n",
    "    if y is None or m is None or d is None:\n",
    "        return None\n",
    "\n",
    "    if y < 100:\n",
    "        y = _expand_year(y)\n",
    "\n",
    "    if not (1 <= m <= 12 and 1 <= d <= 31 and 1900 <= y <= 2100):\n",
    "        return None\n",
    "\n",
    "    if not _valid_date(y, m, d):\n",
    "        return None\n",
    "\n",
    "    return _fmt_ddmmyyyy(y, m, d)\n",
    "\n",
    "def parse_date(text):\n",
    "    \"\"\"\n",
    "    Extract the first valid date found in `text` and return \"DD/MM/YYYY\".\n",
    "    Returns None if no date can be parsed.\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return None\n",
    "\n",
    "    # Pre-clean ordinals like 21st → 21 (helps numeric patterns)\n",
    "    cleaned = _strip_ordinals(text)\n",
    "\n",
    "    candidates = []\n",
    "\n",
    "    def add_candidates(pattern, kind):\n",
    "        for m in pattern.finditer(cleaned):\n",
    "            start = m.start()\n",
    "            gd = m.groupdict()\n",
    "            # unify keys to y, m, d\n",
    "            if \"month\" in gd:\n",
    "                month_name = gd.get(\"month\")\n",
    "                d = gd.get(\"d\")\n",
    "                y = gd.get(\"y\")\n",
    "                s = _try_build_date_from_parts(y_str=y, d_str=d, month_name=month_name)\n",
    "            else:\n",
    "                y = gd.get(\"y\"); m_ = gd.get(\"m\"); d = gd.get(\"d\")\n",
    "                s = _try_build_date_from_parts(y_str=y, m_str=m_, d_str=d)\n",
    "            if s:\n",
    "                candidates.append((start, kind, s))\n",
    "\n",
    "    # Prefer textual dates first (less ambiguous), then ISO, then numeric\n",
    "    add_candidates(PAT_TXT_DMY, \"txt_dmy\")\n",
    "    add_candidates(PAT_TXT_MDY, \"txt_mdy\")\n",
    "    add_candidates(PAT_TXT_YMD, \"txt_ymd\")\n",
    "    add_candidates(PAT_TXT_COMPACT_DMY, \"txt_compact_dmy\")\n",
    "    add_candidates(PAT_TXT_COMPACT_MDY, \"txt_compact_mdy\")\n",
    "    add_candidates(PAT_ISO_YMD, \"iso_ymd\")\n",
    "    add_candidates(PAT_NUM_DMY, \"num_dmy\")\n",
    "    add_candidates(PAT_NUM_MDY, \"num_mdy\")\n",
    "\n",
    "    if not candidates:\n",
    "        return None\n",
    "\n",
    "    # If multiple, choose the earliest occurrence in text.\n",
    "    candidates.sort(key=lambda x: x[0])\n",
    "    earliest_start = candidates[0][0]\n",
    "    tied = [c for c in candidates if c[0] == earliest_start]\n",
    "\n",
    "    if len(tied) == 1:\n",
    "        return tied[0][2]\n",
    "\n",
    "    # Preference within ties: textual first, then iso, then numeric; within numeric prefer DMY.\n",
    "    kind_priority = {\n",
    "        \"txt_dmy\": 1, \"txt_mdy\": 1, \"txt_ymd\": 1, \"txt_compact_dmy\": 1, \"txt_compact_mdy\": 1,\n",
    "        \"iso_ymd\": 2,\n",
    "        \"num_dmy\": 3, \"num_mdy\": 4\n",
    "    }\n",
    "    tied.sort(key=lambda x: kind_priority.get(x[1], 9))\n",
    "    return tied[0][2]\n",
    "\n",
    "# -----------------------------\n",
    "# CSV I/O\n",
    "# -----------------------------\n",
    "def read_csv_rows(path):\n",
    "    \"\"\"\n",
    "    Yield rows from a local CSV path as dicts (if header) or lists (no header).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"CSV not found: {path}\")\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "        sample = f.read(2048)\n",
    "        f.seek(0)\n",
    "        has_header = False\n",
    "        try:\n",
    "            has_header = csv.Sniffer().has_header(sample)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        if has_header:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                yield row\n",
    "        else:\n",
    "            reader = csv.reader(f)\n",
    "            for row in reader:\n",
    "                yield row\n",
    "\n",
    "def pick_text_from_row(row):\n",
    "    \"\"\"\n",
    "    Try to get the text field from a CSV row (dict or list).\n",
    "    Looks for common column names; falls back to first cell.\n",
    "    \"\"\"\n",
    "    if isinstance(row, dict):\n",
    "        for key in row.keys():\n",
    "            if key.lower() in {\"text\", \"sentence\", \"input\", \"review\", \"utterance\", \"query\", \"content\"}:\n",
    "                return row[key]\n",
    "        # Otherwise, join all values into one string\n",
    "        return \" \".join(str(v) for v in row.values())\n",
    "    elif isinstance(row, (list, tuple)):\n",
    "        return row[0] if row else \"\"\n",
    "    else:\n",
    "        return str(row)\n",
    "\n",
    "def process_csv(input_path, output_path=\"parsed_dates_output.csv\"):\n",
    "    rows = list(read_csv_rows(input_path))\n",
    "    out_rows = []\n",
    "    for row in rows:\n",
    "        text = pick_text_from_row(row)\n",
    "        parsed = parse_date(text)\n",
    "        out_rows.append({\"text\": text, \"parsed_date\": parsed if parsed else \"\"})\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"text\", \"parsed_date\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(out_rows)\n",
    "\n",
    "    print(f\"[done] Read {len(rows)} rows from: {input_path}\")\n",
    "    print(f\"[done] Wrote results to: {output_path}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Run (reads your CSV_PATH)\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    if not CSV_PATH or CSV_PATH.strip() == \"/path/to/your/date_parser_testcases.csv\":\n",
    "        print(\"Please set CSV_PATH at the top of the script to your CSV file path.\")\n",
    "    else:\n",
    "        process_csv(CSV_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69adc626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/06/2024\n",
      "07/06/2023\n",
      "21/06/2024\n",
      "21/06/2024\n"
     ]
    }
   ],
   "source": [
    "# --- single-text inference examples ---\n",
    "print(parse_date(\"I went to London on 21st June, 2024\"))     # -> 21/06/2024\n",
    "print(parse_date(\"Meet me June 7, 2023\"))                    # -> 07/06/2023\n",
    "print(parse_date(\"Event: 2024-06-21\"))                       # -> 21/06/2024\n",
    "print(parse_date(\"She wrote 06/21/2024\"))                    # -> 21/06/2024 (prefers D/M/Y on ambiguous)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81242c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He gave her his book.  ->  She gave his her book.\n",
      "SHE SAID HE'LL HELP HER.  ->  HE SAID SHE'LL HELP HIM.\n",
      "The book is his.  ->  The book is hers.\n",
      "I told her that it was important.  ->  I told him that it was important.\n",
      "He’s sure she’d finish before him.  ->  She's sure he'd finish before her.\n",
      "Her idea and her OWN plan were approved.  ->  His idea and his OWN plan were approved.\n",
      "He hurt himself; she blamed herself.  ->  She hurt herself; he blamed himself.\n",
      "She will see him on Friday.  ->  He will see her on Friday.\n",
      "He will see her in LA.  ->  She will see him in LA.\n",
      "\n",
      "[info] Reading CSV: C:/Users/jaypr/Downloads/pronoun_testcases (1).csv\n",
      "[done] Wrote 27 rows to pronoun_transform_output.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Rules-based Gendered Pronoun Transformer (pure Python + regex)\n",
    "\n",
    "Goal:\n",
    "  Swap gendered pronouns in a sentence from one gender to the opposite while\n",
    "  preserving grammatical correctness and meaning as much as possible using rules.\n",
    "\n",
    "Examples:\n",
    "  Input : \"He gave her his book.\"\n",
    "  Mode  : \"swap\"\n",
    "  Output: \"She gave him her book.\"\n",
    "\n",
    "What it handles:\n",
    "  • Subject/Object pronouns: he ↔ she, him ↔ her\n",
    "  • Possessives (determiner vs pronoun): his (det) ↔ her (det), his (pronoun) ↔ hers, her (det) ↔ his, hers ↔ his\n",
    "  • Reflexives: himself ↔ herself\n",
    "  • Common contractions: he's ↔ she's, he'll ↔ she'll, he'd ↔ she'd (both straight and curly apostrophes)\n",
    "  • Case preservation: \"He\" → \"She\"; \"HE\" → \"SHE\"\n",
    "  • Basic disambiguation for \"his\" / \"her\" as determiner vs pronoun:\n",
    "      - If next meaningful word is noun-like → treat as determiner (e.g., \"his book\" → \"her book\")\n",
    "      - If at end / before punctuation or a function word → treat as pronoun (e.g., \"The book is his.\" → \"The book is hers.\")\n",
    "      - If \"her\" is followed by preposition/conjunction/etc. → treat as object pronoun (\"I told her that...\" → \"...him that...\")\n",
    "\n",
    "Modes:\n",
    "  - \"swap\" (default): swap both male↔female forms throughout (this matches the example).\n",
    "  - \"to_female\": change male→female only, leave female as-is.\n",
    "  - \"to_male\"  : change female→male only, leave male as-is.\n",
    "\n",
    "CSV (optional):\n",
    "  - If you set CSV_PATH to a file (with a column named \"text\", or else the first column is used),\n",
    "    the script will transform each row and write \"pronoun_transform_output.csv\".\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "\n",
    "# ===================== USER CONFIG (optional) =====================\n",
    "CSV_PATH = \"C:/Users/jaypr/Downloads/pronoun_testcases (1).csv\"  # e.g., r\"C:\\Users\\you\\Downloads\\pronoun_testcases.csv\"\n",
    "# If left as None, the script won't try to read/write CSV.\n",
    "# If you uploaded to /mnt/data, you can set:\n",
    "# CSV_PATH = r\"/mnt/data/pronoun_testcases (1).csv\"\n",
    "MODE = \"swap\"    # \"swap\" (default), \"to_female\", or \"to_male\"\n",
    "# =================================================================\n",
    "\n",
    "# --- Small word lists used for \"his/her\" disambiguation ---\n",
    "_PREPOSITIONS = {\n",
    "    \"at\",\"on\",\"in\",\"by\",\"for\",\"with\",\"about\",\"against\",\"between\",\"into\",\"through\",\n",
    "    \"during\",\"before\",\"after\",\"above\",\"below\",\"to\",\"from\",\"up\",\"down\",\"of\",\"off\",\n",
    "    \"over\",\"under\",\"again\",\"further\",\"then\",\"once\",\"as\",\"per\",\"via\",\"within\",\"without\",\"onto\",\"upon\"\n",
    "}\n",
    "_AUX_OR_FUNCTION = {\n",
    "    # Auxiliaries / copulas\n",
    "    \"is\",\"am\",\"are\",\"was\",\"were\",\"be\",\"being\",\"been\",\n",
    "    \"has\",\"have\",\"had\",\"do\",\"does\",\"did\",\"can\",\"could\",\"will\",\"would\",\"shall\",\"should\",\"may\",\"might\",\"must\",\n",
    "    # Conjunctions / determiners / wh-words / misc function words\n",
    "    \"that\",\"this\",\"these\",\"those\",\"who\",\"whom\",\"whose\",\"which\",\"what\",\"when\",\"where\",\"why\",\"how\",\n",
    "    \"and\",\"or\",\"but\",\"nor\",\"so\",\"yet\",\"if\",\"than\",\"then\",\"because\",\"although\",\"though\",\"while\",\"whereas\"\n",
    "}\n",
    "# Words that often appear after possessive determiners (still treat as determiner)\n",
    "_DET_FOLLOWERS = {\"own\",\"same\",\"entire\",\"whole\",\"only\",\"former\",\"latter\"}\n",
    "\n",
    "# Regex tokenization: preserve words (incl. contractions) vs whitespace vs punctuation\n",
    "# - words: letters + optional apostrophe + letters, e.g., he's, she’s\n",
    "# - numbers allowed but ignored for pronoun logic\n",
    "_TOKEN_RE = re.compile(r\"\\s+|[A-Za-z]+(?:[’'][A-Za-z]+)?|[0-9]+|[^\\s]\")\n",
    "\n",
    "def _is_word(tok: str) -> bool:\n",
    "    return bool(re.fullmatch(r\"[A-Za-z]+(?:[’'][A-Za-z]+)?\", tok))\n",
    "\n",
    "def _lower_ascii(s: str) -> str:\n",
    "    # Normalize to lowercase and normalize curly apostrophes to straight for matching\n",
    "    return s.replace(\"’\", \"'\").lower()\n",
    "\n",
    "def _preserve_case(src: str, repl: str) -> str:\n",
    "    # Preserve all-caps, Titlecase, or lowercase style of the source token\n",
    "    if src.isupper():\n",
    "        return repl.upper()\n",
    "    if src[:1].isupper() and src[1:].islower():\n",
    "        return repl.capitalize()\n",
    "    return repl\n",
    "\n",
    "def _next_meaningful_word(tokens: List[str], i: int) -> Tuple[int, str]:\n",
    "    \"\"\"\n",
    "    Return (index, next_word_lower) for the next alphabetic token after index i,\n",
    "    skipping whitespace and punctuation. Returns (-1, \"\") if none found.\n",
    "    \"\"\"\n",
    "    j = i + 1\n",
    "    while j < len(tokens):\n",
    "        t = tokens[j]\n",
    "        if _is_word(t):\n",
    "            return j, _lower_ascii(t)\n",
    "        # skip everything else (spaces, punctuation, numbers)\n",
    "        j += 1\n",
    "    return -1, \"\"\n",
    "\n",
    "def _looks_like_determiner(tokens: List[str], i: int) -> bool:\n",
    "    \"\"\"\n",
    "    Heuristic: treat 'his'/'her' as a possessive determiner if a plausible noun-like word follows.\n",
    "    If next meaningful word is:\n",
    "      - in prepositions / auxiliaries / function words -> NOT determiner (so treat as pronoun/object)\n",
    "      - in DET_FOLLOWERS (e.g., 'own') -> determiner\n",
    "      - ends with 'ing' (gerund often acts as noun) -> determiner\n",
    "      - otherwise: determiner\n",
    "    \"\"\"\n",
    "    j, nxt = _next_meaningful_word(tokens, i)\n",
    "    if j == -1 or not nxt:\n",
    "        return False  # end of sentence → likely pronoun (e.g., \"The book is his.\")\n",
    "    if nxt in _PREPOSITIONS or nxt in _AUX_OR_FUNCTION:\n",
    "        return False\n",
    "    if nxt in _DET_FOLLOWERS:\n",
    "        return True\n",
    "    if nxt.endswith(\"ing\"):  # often a noun/gerundial usage (\"his cooking\", \"her singing\")\n",
    "        return True\n",
    "    # fallback: assume it modifies a noun\n",
    "    return True\n",
    "\n",
    "def transform_pronouns(text: str, mode: str = \"swap\") -> str:\n",
    "    \"\"\"\n",
    "    Transform gendered pronouns according to `mode`:\n",
    "      - \"swap\":      male <-> female everywhere (default; matches the example)\n",
    "      - \"to_female\": only male -> female\n",
    "      - \"to_male\":   only female -> male\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "\n",
    "    tokens = _TOKEN_RE.findall(text)\n",
    "    out = []\n",
    "\n",
    "    for i, tok in enumerate(tokens):\n",
    "        if not _is_word(tok):\n",
    "            out.append(tok)\n",
    "            continue\n",
    "\n",
    "        base = _lower_ascii(tok)\n",
    "\n",
    "        # --- Reflexives ---\n",
    "        if base == \"himself\":\n",
    "            repl = \"herself\"\n",
    "            if mode == \"to_male\": repl = None\n",
    "        elif base == \"herself\":\n",
    "            repl = \"himself\"\n",
    "            if mode == \"to_female\": repl = None\n",
    "\n",
    "        # --- Contractions ---\n",
    "        elif base in {\"he's\",\"he’ll\",\"he'll\",\"he’d\",\"he'd\"}:\n",
    "            key = base.replace(\"’\", \"'\")\n",
    "            c_map = {\"he's\": \"she's\", \"he'll\": \"she'll\", \"he'd\": \"she'd\"}\n",
    "            repl = c_map.get(key)\n",
    "            if mode == \"to_male\": repl = None\n",
    "        elif base in {\"she's\",\"she’ll\",\"she'll\",\"she’d\",\"she'd\"}:\n",
    "            key = base.replace(\"’\", \"'\")\n",
    "            c_map = {\"she's\": \"he's\", \"she'll\": \"he'll\", \"she'd\": \"he'd\"}\n",
    "            repl = c_map.get(key)\n",
    "            if mode == \"to_female\": repl = None\n",
    "\n",
    "        # --- Simple subject/object pronouns ---\n",
    "        elif base == \"he\":\n",
    "            repl = \"she\"\n",
    "            if mode == \"to_male\": repl = None\n",
    "        elif base == \"she\":\n",
    "            repl = \"he\"\n",
    "            if mode == \"to_female\": repl = None\n",
    "        elif base == \"him\":\n",
    "            repl = \"her\"\n",
    "            if mode == \"to_male\": repl = None\n",
    "        elif base == \"her\":\n",
    "            # 'her' can be object pronoun or possessive determiner\n",
    "            if _looks_like_determiner(tokens, i):\n",
    "                # determiner 'her' → 'his'\n",
    "                repl = \"his\"\n",
    "                if mode == \"to_female\": repl = None  # leave as-is if only converting male→female\n",
    "            else:\n",
    "                # object pronoun 'her' → 'him'\n",
    "                repl = \"him\"\n",
    "                if mode == \"to_female\": repl = None\n",
    "\n",
    "        # --- Possessives ---\n",
    "        elif base == \"his\":\n",
    "            # 'his' can be possessive determiner or possessive pronoun\n",
    "            if _looks_like_determiner(tokens, i):\n",
    "                # determiner 'his' → 'her'\n",
    "                repl = \"her\"\n",
    "            else:\n",
    "                # pronoun 'his' → 'hers'\n",
    "                repl = \"hers\"\n",
    "            if mode == \"to_male\": repl = None\n",
    "        elif base == \"hers\":\n",
    "            repl = \"his\"\n",
    "            if mode == \"to_female\": repl = None\n",
    "\n",
    "        else:\n",
    "            repl = None\n",
    "\n",
    "        if repl is None:\n",
    "            out.append(tok)\n",
    "        else:\n",
    "            out.append(_preserve_case(tok, repl))\n",
    "\n",
    "    return \"\".join(out)\n",
    "\n",
    "# ---------------- CSV Helpers (optional) ----------------\n",
    "def _read_csv_rows(path: str):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"CSV not found: {path}\")\n",
    "    with open(path, \"r\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "        sample = f.read(2048)\n",
    "        f.seek(0)\n",
    "        has_header = False\n",
    "        try:\n",
    "            has_header = csv.Sniffer().has_header(sample)\n",
    "        except Exception:\n",
    "            pass\n",
    "        if has_header:\n",
    "            reader = csv.DictReader(f)\n",
    "            rows = list(reader)\n",
    "            # Guess column\n",
    "            text_col = None\n",
    "            if rows:\n",
    "                keys = [k for k in rows[0].keys()]\n",
    "                # prefer a 'text' column name if present\n",
    "                for k in keys:\n",
    "                    if k.lower() in {\"text\",\"sentence\",\"input\",\"utterance\"}:\n",
    "                        text_col = k\n",
    "                        break\n",
    "                if text_col is None:\n",
    "                    text_col = keys[0]\n",
    "            return [r.get(text_col, \"\") for r in rows]\n",
    "        else:\n",
    "            return [row[0] if row else \"\" for row in csv.reader(f)]\n",
    "\n",
    "def _write_csv_rows(path: str, rows: List[dict], fieldnames: List[str]):\n",
    "    with open(path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        w.writeheader()\n",
    "        w.writerows(rows)\n",
    "\n",
    "# ---------------- Demo / Main ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Quick sanity checks\n",
    "    examples = [\n",
    "        ('He gave her his book.', \"swap\"),\n",
    "        (\"SHE SAID HE'LL HELP HER.\", \"swap\"),\n",
    "        (\"The book is his.\", \"swap\"),\n",
    "        (\"I told her that it was important.\", \"swap\"),\n",
    "        (\"He’s sure she’d finish before him.\", \"swap\"),\n",
    "        (\"Her idea and her OWN plan were approved.\", \"swap\"),\n",
    "        (\"He hurt himself; she blamed herself.\", \"swap\"),\n",
    "        (\"She will see him on Friday.\", \"swap\"),\n",
    "        (\"He will see her in LA.\", \"swap\"),\n",
    "    ]\n",
    "    for s, m in examples:\n",
    "        print(f\"{s}  ->  {transform_pronouns(s, m)}\")\n",
    "\n",
    "    # Optional CSV processing\n",
    "    if CSV_PATH:\n",
    "        print(f\"\\n[info] Reading CSV: {CSV_PATH}\")\n",
    "        texts = _read_csv_rows(CSV_PATH)\n",
    "        results = []\n",
    "        for t in texts:\n",
    "            results.append({\"text\": t, \"transformed\": transform_pronouns(t, MODE)})\n",
    "        out_path = \"pronoun_transform_output.csv\"\n",
    "        _write_csv_rows(out_path, results, fieldnames=[\"text\",\"transformed\"])\n",
    "        print(f\"[done] Wrote {len(results)} rows to {out_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
